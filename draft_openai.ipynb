{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install openai\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï New game started.\n",
      "\n",
      "üå≤ Welcome to the AI Dungeon! üå≤\n",
      "\n",
      "üìñ Story so far:\n",
      "Ihno awakens in a dark forest. A mysterious figure approaches.\n",
      "\n",
      "üßç Ihno's Inventory: ['torch']\n",
      "‚ù§Ô∏è Stats: {'health': 100, 'strength': 10, 'gold': 5} | üéØ Difficulty: Medium\n"
     ]
    },
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAPIRemovedInV1\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 124\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# Context for model\u001b[39;00m\n\u001b[32m    123\u001b[39m recent_context = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(game_memory[-\u001b[32m6\u001b[39m:])\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m new_story = \u001b[43mgenerate_story\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecent_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdifficulty\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m context += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mplayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplayer_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnew_story\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    127\u001b[39m game_memory.append(new_story)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mgenerate_story\u001b[39m\u001b[34m(context, player_input, difficulty)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_story\u001b[39m(context, player_input, difficulty):\n\u001b[32m     21\u001b[39m     messages = [\n\u001b[32m     22\u001b[39m         {\n\u001b[32m     23\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: context + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mplayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplayer_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m},\n\u001b[32m     31\u001b[39m     ]\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     response = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-3.5-turbo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m250\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m].strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\openai\\lib\\_old_api.py:39\u001b[39m, in \u001b[36mAPIRemovedInV1Proxy.__call__\u001b[39m\u001b[34m(self, *_args, **_kwargs)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *_args: Any, **_kwargs: Any) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol=\u001b[38;5;28mself\u001b[39m._symbol)\n",
      "\u001b[31mAPIRemovedInV1\u001b[39m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env if available\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Game variables\n",
    "player_name = \"Ihno\"\n",
    "save_file = \"game_state.json\"\n",
    "context = \"\"\n",
    "game_memory = []\n",
    "player_stats = {}\n",
    "inventory = []\n",
    "difficulty = 1\n",
    "\n",
    "# Story generation using ChatGPT\n",
    "def generate_story(context, player_input, difficulty):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                f\"You are a text-based fantasy adventure game. \"\n",
    "                f\"Difficulty is {['Easy', 'Medium', 'Hard'][difficulty]}. \"\n",
    "                f\"Be creative, descriptive, and immersive. Respond with only the next part of the story, not instructions.\"\n",
    "            ),\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": context + f\"\\n{player_name}: {player_input}\"},\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0.9,\n",
    "        max_tokens=250,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"].strip()\n",
    "\n",
    "# Difficulty adjustment\n",
    "def adjust_difficulty(player_input):\n",
    "    global difficulty\n",
    "    if any(word in player_input.lower() for word in [\"attack\", \"fight\", \"battle\"]):\n",
    "        difficulty = min(difficulty + 1, 2)\n",
    "    elif any(word in player_input.lower() for word in [\"run\", \"talk\", \"hide\"]):\n",
    "        difficulty = max(difficulty - 1, 0)\n",
    "    return difficulty\n",
    "\n",
    "# Save/load/delete\n",
    "def save_game():\n",
    "    with open(save_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"context\": context,\n",
    "            \"stats\": player_stats,\n",
    "            \"inventory\": inventory,\n",
    "            \"difficulty\": difficulty,\n",
    "            \"memory\": game_memory\n",
    "        }, f, indent=4)\n",
    "\n",
    "def delete_save():\n",
    "    if os.path.exists(save_file):\n",
    "        os.remove(save_file)\n",
    "        print(\"üóëÔ∏è Previous save data deleted.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No save file found to delete.\")\n",
    "\n",
    "# Start menu\n",
    "game_running = False\n",
    "while not game_running:\n",
    "    choice = input(\"\\nüíæ Load game (l), üÜï New game (n), üóëÔ∏è Delete save (d), or ‚ùå Quit (q)? \").strip().lower()\n",
    "    if choice == 'l':\n",
    "        if os.path.exists(save_file):\n",
    "            with open(save_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                game_state = json.load(f)\n",
    "                context = game_state[\"context\"]\n",
    "                player_stats = game_state[\"stats\"]\n",
    "                inventory = game_state[\"inventory\"]\n",
    "                difficulty = game_state[\"difficulty\"]\n",
    "                game_memory = game_state[\"memory\"]\n",
    "            print(\"‚úÖ Game loaded.\")\n",
    "            game_running = True\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No saved game found.\")\n",
    "    elif choice == 'n':\n",
    "        context = f\"{player_name} awakens in a dark forest. A mysterious figure approaches.\"\n",
    "        game_memory = [context]\n",
    "        player_stats = {\"health\": 100, \"strength\": 10, \"gold\": 5}\n",
    "        inventory = [\"torch\"]\n",
    "        difficulty = 1\n",
    "        print(\"üÜï New game started.\")\n",
    "        game_running = True\n",
    "    elif choice == 'd':\n",
    "        delete_save()\n",
    "    elif choice == 'q':\n",
    "        print(\"üõë Exiting game.\")\n",
    "        exit()\n",
    "    else:\n",
    "        print(\"‚ùì Invalid choice.\")\n",
    "\n",
    "# --- GAME LOOP ---\n",
    "print(\"\\nüå≤ Welcome to the AI Dungeon! üå≤\")\n",
    "while True:\n",
    "    print(\"\\nüìñ Story so far:\")\n",
    "    print(context)\n",
    "    print(f\"\\nüßç {player_name}'s Inventory: {inventory}\")\n",
    "    print(f\"‚ù§Ô∏è Stats: {player_stats} | üéØ Difficulty: {['Easy', 'Medium', 'Hard'][difficulty]}\")\n",
    "\n",
    "    player_input = input(\"\\nüí¨ What do you do? (type 'save', 'quit' or your action): \").strip()\n",
    "\n",
    "    if player_input.lower() == 'quit':\n",
    "        print(\"üõë Game ended. Thanks for playing!\")\n",
    "        break\n",
    "    elif player_input.lower() == 'save':\n",
    "        save_game()\n",
    "        print(\"üíæ Game saved.\")\n",
    "        continue\n",
    "\n",
    "    game_memory.append(f\"{player_name}: {player_input}\")\n",
    "    adjust_difficulty(player_input)\n",
    "\n",
    "    # Context for model\n",
    "    recent_context = \"\\n\".join(game_memory[-6:])\n",
    "    new_story = generate_story(recent_context, player_input, difficulty)\n",
    "\n",
    "    context += f\"\\n{player_name}: {player_input}\\n{new_story}\"\n",
    "    game_memory.append(new_story)\n",
    "\n",
    "    # Log story\n",
    "    with open(\"story_log.txt\", \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(f\"{player_name}: {player_input}\\n{new_story}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draft 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï New game started.\n",
      "\n",
      "üå≤ Welcome to the AI Dungeon! üå≤\n",
      "\n",
      "üìñ Story so far:\n",
      "Ihno awakens in a dark forest. A mysterious figure approaches.\n",
      "\n",
      "üßç Ihno's Inventory: ['torch']\n",
      "‚ù§Ô∏è Stats: {'health': 100, 'strength': 10, 'gold': 5} | üéØ Difficulty: Medium\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 130\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Context for model\u001b[39;00m\n\u001b[32m    129\u001b[39m recent_context = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(game_memory[-\u001b[32m6\u001b[39m:])\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m new_story = \u001b[43mgenerate_story\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecent_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdifficulty\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m context += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mplayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplayer_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnew_story\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m game_memory.append(new_story)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mgenerate_story\u001b[39m\u001b[34m(context, player_input, difficulty)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_story\u001b[39m(context, player_input, difficulty):\n\u001b[32m     23\u001b[39m     messages = [\n\u001b[32m     24\u001b[39m         {\n\u001b[32m     25\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m         },\n\u001b[32m     36\u001b[39m     ]\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-3.5-turbo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m250\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message.content.strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:279\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:914\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    871\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    872\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    873\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    911\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    912\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    913\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1247\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1235\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1242\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1243\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1244\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1245\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1246\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1247\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\openai\\_base_client.py:920\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    918\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m920\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1013\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_retry(err.response):\n\u001b[32m   1012\u001b[39m     err.response.close()\n\u001b[32m-> \u001b[39m\u001b[32m1013\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43merr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[32m   1023\u001b[39m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err.response.is_closed:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1062\u001b[39m, in \u001b[36mSyncAPIClient._retry_request\u001b[39m\u001b[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[39m\n\u001b[32m   1058\u001b[39m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[32m   1059\u001b[39m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[32m   1060\u001b[39m time.sleep(timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1013\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_retry(err.response):\n\u001b[32m   1012\u001b[39m     err.response.close()\n\u001b[32m-> \u001b[39m\u001b[32m1013\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43merr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[32m   1023\u001b[39m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err.response.is_closed:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1062\u001b[39m, in \u001b[36mSyncAPIClient._retry_request\u001b[39m\u001b[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[39m\n\u001b[32m   1058\u001b[39m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[32m   1059\u001b[39m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[32m   1060\u001b[39m time.sleep(timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1028\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1025\u001b[39m         err.response.read()\n\u001b[32m   1027\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1028\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1030\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1031\u001b[39m     cast_to=cast_to,\n\u001b[32m   1032\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1036\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1037\u001b[39m )\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env if available\n",
    "load_dotenv()\n",
    "\n",
    "# Create OpenAI client with your API key\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Game variables\n",
    "player_name = \"Ihno\"\n",
    "save_file = \"game_state.json\"\n",
    "context = \"\"\n",
    "game_memory = []\n",
    "player_stats = {}\n",
    "inventory = []\n",
    "difficulty = 1\n",
    "\n",
    "# Story generation using ChatGPT\n",
    "def generate_story(context, player_input, difficulty):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                f\"You are a text-based fantasy adventure game. \"\n",
    "                f\"Difficulty is {['Easy', 'Medium', 'Hard'][difficulty]}. \"\n",
    "                f\"Be creative, descriptive, and immersive. Respond with only the next part of the story, not instructions.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": context + f\"\\n{player_name}: {player_input}\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0.9,\n",
    "        max_tokens=250,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Difficulty adjustment\n",
    "def adjust_difficulty(player_input):\n",
    "    global difficulty\n",
    "    if any(word in player_input.lower() for word in [\"attack\", \"fight\", \"battle\"]):\n",
    "        difficulty = min(difficulty + 1, 2)\n",
    "    elif any(word in player_input.lower() for word in [\"run\", \"talk\", \"hide\"]):\n",
    "        difficulty = max(difficulty - 1, 0)\n",
    "    return difficulty\n",
    "\n",
    "# Save/load/delete\n",
    "def save_game():\n",
    "    with open(save_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"context\": context,\n",
    "            \"stats\": player_stats,\n",
    "            \"inventory\": inventory,\n",
    "            \"difficulty\": difficulty,\n",
    "            \"memory\": game_memory\n",
    "        }, f, indent=4)\n",
    "\n",
    "def delete_save():\n",
    "    if os.path.exists(save_file):\n",
    "        os.remove(save_file)\n",
    "        print(\"üóëÔ∏è Previous save data deleted.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No save file found to delete.\")\n",
    "\n",
    "# Start menu\n",
    "game_running = False\n",
    "while not game_running:\n",
    "    choice = input(\"\\nüíæ Load game (l), üÜï New game (n), üóëÔ∏è Delete save (d), or ‚ùå Quit (q)? \").strip().lower()\n",
    "    if choice == 'l':\n",
    "        if os.path.exists(save_file):\n",
    "            with open(save_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                game_state = json.load(f)\n",
    "                context = game_state[\"context\"]\n",
    "                player_stats = game_state[\"stats\"]\n",
    "                inventory = game_state[\"inventory\"]\n",
    "                difficulty = game_state[\"difficulty\"]\n",
    "                game_memory = game_state[\"memory\"]\n",
    "            print(\"‚úÖ Game loaded.\")\n",
    "            game_running = True\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No saved game found.\")\n",
    "    elif choice == 'n':\n",
    "        context = f\"{player_name} awakens in a dark forest. A mysterious figure approaches.\"\n",
    "        game_memory = [context]\n",
    "        player_stats = {\"health\": 100, \"strength\": 10, \"gold\": 5}\n",
    "        inventory = [\"torch\"]\n",
    "        difficulty = 1\n",
    "        print(\"üÜï New game started.\")\n",
    "        game_running = True\n",
    "    elif choice == 'd':\n",
    "        delete_save()\n",
    "    elif choice == 'q':\n",
    "        print(\"üõë Exiting game.\")\n",
    "        exit()\n",
    "    else:\n",
    "        print(\"‚ùì Invalid choice.\")\n",
    "\n",
    "# --- GAME LOOP ---\n",
    "print(\"\\nüå≤ Welcome to the AI Dungeon! üå≤\")\n",
    "while True:\n",
    "    print(\"\\nüìñ Story so far:\")\n",
    "    print(context)\n",
    "    print(f\"\\nüßç {player_name}'s Inventory: {inventory}\")\n",
    "    print(f\"‚ù§Ô∏è Stats: {player_stats} | üéØ Difficulty: {['Easy', 'Medium', 'Hard'][difficulty]}\")\n",
    "\n",
    "    player_input = input(\"\\nüí¨ What do you do? (type 'save', 'quit' or your action): \").strip()\n",
    "\n",
    "    if player_input.lower() == 'quit':\n",
    "        print(\"üõë Game ended. Thanks for playing!\")\n",
    "        break\n",
    "    elif player_input.lower() == 'save':\n",
    "        save_game()\n",
    "        print(\"üíæ Game saved.\")\n",
    "        continue\n",
    "\n",
    "    game_memory.append(f\"{player_name}: {player_input}\")\n",
    "    adjust_difficulty(player_input)\n",
    "\n",
    "    # Context for model\n",
    "    recent_context = \"\\n\".join(game_memory[-6:])\n",
    "    new_story = generate_story(recent_context, player_input, difficulty)\n",
    "\n",
    "    context += f\"\\n{player_name}: {player_input}\\n{new_story}\"\n",
    "    game_memory.append(new_story)\n",
    "\n",
    "    # Log story\n",
    "    with open(\"story_log.txt\", \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(f\"{player_name}: {player_input}\\n{new_story}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draft 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï New game started.\n",
      "\n",
      "üå≤ Welcome to the AI Dungeon! üå≤\n",
      "\n",
      "üìñ Story so far:\n",
      "Ihno awakens in a dark forest. A mysterious figure approaches.\n",
      "\n",
      "üßç Ihno's Inventory: ['torch']\n",
      "‚ù§Ô∏è Stats: {'health': 100, 'strength': 10, 'gold': 5} | üéØ Difficulty: Medium\n",
      "\n",
      "üìñ Story so far:\n",
      "Ihno awakens in a dark forest. A mysterious figure approaches.\n",
      "Ihno: Helle sir.\n",
      "Ihno Awakenings\n",
      "Level Design: Not too difficult\n",
      "Control: A-C\n",
      "Graphics: OK\n",
      "Soundtrack: OK\n",
      "Ihno: You have awoken in a forest. There is a figure who asks you to guide him as you seek his treasure. Help him reach the treasures of the lands.\n",
      "How to Play: Go to the nearest map marker in the world and navigate the map. Take turns playing as Helle and guiding him as he explores.\n",
      "A-C-C-A-B-D-A\n",
      "A-C-C-A-B-D-A\n",
      "Game Features:\n",
      "Ihno features a number of optional actions.\n",
      "There are optional actions to move the map marker\n",
      "\n",
      "üßç Ihno's Inventory: ['torch']\n",
      "‚ù§Ô∏è Stats: {'health': 100, 'strength': 10, 'gold': 5} | üéØ Difficulty: Medium\n",
      "\n",
      "üìñ Story so far:\n",
      "Ihno awakens in a dark forest. A mysterious figure approaches.\n",
      "Ihno: Helle sir.\n",
      "Ihno Awakenings\n",
      "Level Design: Not too difficult\n",
      "Control: A-C\n",
      "Graphics: OK\n",
      "Soundtrack: OK\n",
      "Ihno: You have awoken in a forest. There is a figure who asks you to guide him as you seek his treasure. Help him reach the treasures of the lands.\n",
      "How to Play: Go to the nearest map marker in the world and navigate the map. Take turns playing as Helle and guiding him as he explores.\n",
      "A-C-C-A-B-D-A\n",
      "A-C-C-A-B-D-A\n",
      "Game Features:\n",
      "Ihno features a number of optional actions.\n",
      "There are optional actions to move the map marker\n",
      "Ihno: Where is the map marker in the world? In which direction?\n",
      "\n",
      "\n",
      "üßç Ihno's Inventory: ['torch']\n",
      "‚ù§Ô∏è Stats: {'health': 100, 'strength': 10, 'gold': 5} | üéØ Difficulty: Medium\n",
      "\n",
      "üìñ Story so far:\n",
      "Ihno awakens in a dark forest. A mysterious figure approaches.\n",
      "Ihno: Helle sir.\n",
      "Ihno Awakenings\n",
      "Level Design: Not too difficult\n",
      "Control: A-C\n",
      "Graphics: OK\n",
      "Soundtrack: OK\n",
      "Ihno: You have awoken in a forest. There is a figure who asks you to guide him as you seek his treasure. Help him reach the treasures of the lands.\n",
      "How to Play: Go to the nearest map marker in the world and navigate the map. Take turns playing as Helle and guiding him as he explores.\n",
      "A-C-C-A-B-D-A\n",
      "A-C-C-A-B-D-A\n",
      "Game Features:\n",
      "Ihno features a number of optional actions.\n",
      "There are optional actions to move the map marker\n",
      "Ihno: Where is the map marker in the world? In which direction?\n",
      "\n",
      "Ihno: Where is the map marker?\n",
      "You can choose to play as either Helle or as your character from the main game.\n",
      "Ihno: Which of the following actions do you want to take?\n",
      "A-C-C-A-B-D-A\n",
      "A-C-C-A-B-\n",
      "\n",
      "üßç Ihno's Inventory: ['torch']\n",
      "‚ù§Ô∏è Stats: {'health': 100, 'strength': 10, 'gold': 5} | üéØ Difficulty: Medium\n",
      "\n",
      "üìñ Story so far:\n",
      "Ihno awakens in a dark forest. A mysterious figure approaches.\n",
      "Ihno: Helle sir.\n",
      "Ihno Awakenings\n",
      "Level Design: Not too difficult\n",
      "Control: A-C\n",
      "Graphics: OK\n",
      "Soundtrack: OK\n",
      "Ihno: You have awoken in a forest. There is a figure who asks you to guide him as you seek his treasure. Help him reach the treasures of the lands.\n",
      "How to Play: Go to the nearest map marker in the world and navigate the map. Take turns playing as Helle and guiding him as he explores.\n",
      "A-C-C-A-B-D-A\n",
      "A-C-C-A-B-D-A\n",
      "Game Features:\n",
      "Ihno features a number of optional actions.\n",
      "There are optional actions to move the map marker\n",
      "Ihno: Where is the map marker in the world? In which direction?\n",
      "\n",
      "Ihno: Where is the map marker?\n",
      "You can choose to play as either Helle or as your character from the main game.\n",
      "Ihno: Which of the following actions do you want to take?\n",
      "A-C-C-A-B-D-A\n",
      "A-C-C-A-B-\n",
      "Ihno: A-C-C-A-B-D-A\n",
      "Ihno is an advanced version of the classic adventure game, Ihno Adventure. Players begin in a forest and will have to solve puzzles, find hidden treasure, travel around the world and solve hidden puzzles.\n",
      "Features:\n",
      "Ihno features a number of optional actions.\n",
      "There are optional actions to move the map marker\n",
      "Ihno: Where is the map marker in the world? In which direction?\n",
      "Ihno: Where is the map marker?\n",
      "You can choose to play as Helle or as your character from the main game.\n",
      "Ihno: Which of the following actions do you want to take?\n",
      "A-C-C-A-B-D-A\n",
      "A-C-C\n",
      "\n",
      "üßç Ihno's Inventory: ['torch']\n",
      "‚ù§Ô∏è Stats: {'health': 100, 'strength': 10, 'gold': 5} | üéØ Difficulty: Medium\n",
      "\n",
      "üìñ Story so far:\n",
      "Ihno awakens in a dark forest. A mysterious figure approaches.\n",
      "Ihno: Helle sir.\n",
      "Ihno Awakenings\n",
      "Level Design: Not too difficult\n",
      "Control: A-C\n",
      "Graphics: OK\n",
      "Soundtrack: OK\n",
      "Ihno: You have awoken in a forest. There is a figure who asks you to guide him as you seek his treasure. Help him reach the treasures of the lands.\n",
      "How to Play: Go to the nearest map marker in the world and navigate the map. Take turns playing as Helle and guiding him as he explores.\n",
      "A-C-C-A-B-D-A\n",
      "A-C-C-A-B-D-A\n",
      "Game Features:\n",
      "Ihno features a number of optional actions.\n",
      "There are optional actions to move the map marker\n",
      "Ihno: Where is the map marker in the world? In which direction?\n",
      "\n",
      "Ihno: Where is the map marker?\n",
      "You can choose to play as either Helle or as your character from the main game.\n",
      "Ihno: Which of the following actions do you want to take?\n",
      "A-C-C-A-B-D-A\n",
      "A-C-C-A-B-\n",
      "Ihno: A-C-C-A-B-D-A\n",
      "Ihno is an advanced version of the classic adventure game, Ihno Adventure. Players begin in a forest and will have to solve puzzles, find hidden treasure, travel around the world and solve hidden puzzles.\n",
      "Features:\n",
      "Ihno features a number of optional actions.\n",
      "There are optional actions to move the map marker\n",
      "Ihno: Where is the map marker in the world? In which direction?\n",
      "Ihno: Where is the map marker?\n",
      "You can choose to play as Helle or as your character from the main game.\n",
      "Ihno: Which of the following actions do you want to take?\n",
      "A-C-C-A-B-D-A\n",
      "A-C-C\n",
      "Ihno: explore the forest\n",
      "Ihno Adventure\n",
      "\n",
      "üßç Ihno's Inventory: ['torch']\n",
      "‚ù§Ô∏è Stats: {'health': 100, 'strength': 10, 'gold': 5} | üéØ Difficulty: Medium\n",
      "üõë Game ended. Thanks for playing!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Game variables\n",
    "player_name = \"Ihno\"\n",
    "save_file = \"game_state.json\"\n",
    "context = \"\"\n",
    "game_memory = []\n",
    "player_stats = {}\n",
    "inventory = []\n",
    "difficulty = 1\n",
    "\n",
    "# Story generation using GPT-2\n",
    "def generate_story(context, player_input, difficulty):\n",
    "    prompt = (\n",
    "        f\"You are a text-based fantasy adventure game.\\n\"\n",
    "        f\"Difficulty: {['Easy', 'Medium', 'Hard'][difficulty]}\\n\"\n",
    "        f\"{context}\\n\"\n",
    "        f\"{player_name}: {player_input}\\nGame:\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.9,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    story = output_text.split(\"Game:\")[-1].strip()\n",
    "    return story\n",
    "\n",
    "# Difficulty adjustment\n",
    "def adjust_difficulty(player_input):\n",
    "    global difficulty\n",
    "    if any(word in player_input.lower() for word in [\"attack\", \"fight\", \"battle\"]):\n",
    "        difficulty = min(difficulty + 1, 2)\n",
    "    elif any(word in player_input.lower() for word in [\"run\", \"talk\", \"hide\"]):\n",
    "        difficulty = max(difficulty - 1, 0)\n",
    "    return difficulty\n",
    "\n",
    "# Save/load/delete\n",
    "def save_game():\n",
    "    with open(save_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"context\": context,\n",
    "            \"stats\": player_stats,\n",
    "            \"inventory\": inventory,\n",
    "            \"difficulty\": difficulty,\n",
    "            \"memory\": game_memory\n",
    "        }, f, indent=4)\n",
    "\n",
    "def delete_save():\n",
    "    if os.path.exists(save_file):\n",
    "        os.remove(save_file)\n",
    "        print(\"üóëÔ∏è Previous save data deleted.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No save file found to delete.\")\n",
    "\n",
    "# Start menu\n",
    "game_running = False\n",
    "while not game_running:\n",
    "    choice = input(\"\\nüíæ Load game (l), üÜï New game (n), üóëÔ∏è Delete save (d), or ‚ùå Quit (q)? \").strip().lower()\n",
    "    if choice == 'l':\n",
    "        if os.path.exists(save_file):\n",
    "            with open(save_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                game_state = json.load(f)\n",
    "                context = game_state[\"context\"]\n",
    "                player_stats = game_state[\"stats\"]\n",
    "                inventory = game_state[\"inventory\"]\n",
    "                difficulty = game_state[\"difficulty\"]\n",
    "                game_memory = game_state[\"memory\"]\n",
    "            print(\"‚úÖ Game loaded.\")\n",
    "            game_running = True\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No saved game found.\")\n",
    "    elif choice == 'n':\n",
    "        context = f\"{player_name} awakens in a dark forest. A mysterious figure approaches.\"\n",
    "        game_memory = [context]\n",
    "        player_stats = {\"health\": 100, \"strength\": 10, \"gold\": 5}\n",
    "        inventory = [\"torch\"]\n",
    "        difficulty = 1\n",
    "        print(\"üÜï New game started.\")\n",
    "        game_running = True\n",
    "    elif choice == 'd':\n",
    "        delete_save()\n",
    "    elif choice == 'q':\n",
    "        print(\"üõë Exiting game.\")\n",
    "        exit()\n",
    "    else:\n",
    "        print(\"‚ùì Invalid choice.\")\n",
    "\n",
    "# --- GAME LOOP ---\n",
    "print(\"\\nüå≤ Welcome to the AI Dungeon! üå≤\")\n",
    "while True:\n",
    "    print(\"\\nüìñ Story so far:\")\n",
    "    print(context)\n",
    "    print(f\"\\nüßç {player_name}'s Inventory: {inventory}\")\n",
    "    print(f\"‚ù§Ô∏è Stats: {player_stats} | üéØ Difficulty: {['Easy', 'Medium', 'Hard'][difficulty]}\")\n",
    "\n",
    "    player_input = input(\"\\nüí¨ What do you do? (type 'save', 'quit' or your action): \").strip()\n",
    "\n",
    "    if player_input.lower() == 'quit':\n",
    "        print(\"üõë Game ended. Thanks for playing!\")\n",
    "        break\n",
    "    elif player_input.lower() == 'save':\n",
    "        save_game()\n",
    "        print(\"üíæ Game saved.\")\n",
    "        continue\n",
    "\n",
    "    game_memory.append(f\"{player_name}: {player_input}\")\n",
    "    adjust_difficulty(player_input)\n",
    "\n",
    "    # Context for model\n",
    "    recent_context = \"\\n\".join(game_memory[-6:])\n",
    "    new_story = generate_story(recent_context, player_input, difficulty)\n",
    "\n",
    "    context += f\"\\n{player_name}: {player_input}\\n{new_story}\"\n",
    "    game_memory.append(new_story)\n",
    "\n",
    "    # Log story\n",
    "    with open(\"story_log.txt\", \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(f\"{player_name}: {player_input}\\n{new_story}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draft gpt 2, clear prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï New game started.\n",
      "\n",
      "üå≤ Welcome to the AI Dungeon! üå≤\n",
      "\n",
      "üìñ Story so far:\n",
      "Ihno awakens in a dark forest. A mysterious figure approaches.\n",
      "\n",
      "üßç Ihno's Inventory: ['torch']\n",
      "‚ù§Ô∏è Stats: {'health': 100, 'strength': 10, 'gold': 5} | üéØ Difficulty: Medium\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìñ A strange figure starts to search for me.\n",
      "\n",
      "üßç Ihno's Inventory: ['torch']\n",
      "‚ù§Ô∏è Stats: {'health': 100, 'strength': 10, 'gold': 5} | üéØ Difficulty: Medium\n",
      "\n",
      "\n",
      "üìñ The strange figure looks at me.\n",
      "\n",
      "üßç Ihno's Inventory: ['torch']\n",
      "‚ù§Ô∏è Stats: {'health': 100, 'strength': 10, 'gold': 5} | üéØ Difficulty: Medium\n",
      "\n",
      "\n",
      "üìñ I enter the forest.\n",
      "\n",
      "üßç Ihno's Inventory: ['torch']\n",
      "‚ù§Ô∏è Stats: {'health': 100, 'strength': 10, 'gold': 5} | üéØ Difficulty: Medium\n",
      "\n",
      "\n",
      "üìñ I see a monster, and I run away.\n",
      "\n",
      "üßç Ihno's Inventory: ['torch']\n",
      "‚ù§Ô∏è Stats: {'health': 100, 'strength': 10, 'gold': 5} | üéØ Difficulty: Medium\n",
      "\n",
      "\n",
      "üìñ I see a monster, and I run away.\n",
      "\n",
      "üßç Ihno's Inventory: ['torch']\n",
      "‚ù§Ô∏è Stats: {'health': 100, 'strength': 10, 'gold': 5} | üéØ Difficulty: Medium\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load GPT-2\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Game State\n",
    "player_name = \"Ihno\"\n",
    "inventory = [\"torch\"]\n",
    "stats = {\"health\": 100, \"strength\": 10, \"gold\": 5}\n",
    "difficulty = \"Medium\"\n",
    "story_context = f\"{player_name} awakens in a dark forest. A mysterious figure approaches.\"\n",
    "\n",
    "# Prompt formatting\n",
    "def build_prompt(context, player_input):\n",
    "    return (\n",
    "        \"This is a fantasy text adventure. Continue the story based on the player's actions.\\n\"\n",
    "        f\"Story so far:\\n{context}\\n\"\n",
    "        f\"{player_name}: {player_input}\\n\"\n",
    "        \"Narrator:\"\n",
    "    )\n",
    "\n",
    "# Generate story continuation\n",
    "def generate_response(prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=inputs[\"input_ids\"].shape[1] + max_length,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text[len(prompt):].strip().split(\"\\n\")[0]\n",
    "\n",
    "# Game loop\n",
    "print(\"üÜï New game started.\\n\")\n",
    "print(\"üå≤ Welcome to the AI Dungeon! üå≤\\n\")\n",
    "print(f\"üìñ Story so far:\\n{story_context}\\n\")\n",
    "print(f\"üßç {player_name}'s Inventory: {inventory}\")\n",
    "print(f\"‚ù§Ô∏è Stats: {stats} | üéØ Difficulty: {difficulty}\\n\")\n",
    "\n",
    "while True:\n",
    "    player_input = input(f\"{player_name}: \")\n",
    "\n",
    "    if player_input.lower() in [\"quit\", \"exit\"]:\n",
    "        print(\"üõë Game ended. Thanks for playing!\")\n",
    "        break\n",
    "\n",
    "    prompt = build_prompt(story_context, player_input)\n",
    "    narration = generate_response(prompt)\n",
    "\n",
    "    # Update story context\n",
    "    story_context += f\"\\n{player_name}: {player_input}\\nNarrator: {narration}\"\n",
    "\n",
    "    # Show updated story\n",
    "    print(f\"\\nüìñ {narration}\\n\")\n",
    "    print(f\"üßç {player_name}'s Inventory: {inventory}\")\n",
    "    print(f\"‚ù§Ô∏è Stats: {stats} | üéØ Difficulty: {difficulty}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draft full code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "gpt3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\requests\\models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/gpt3/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRepositoryNotFoundError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:424\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    423\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:961\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m    963\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1068\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1068\u001b[39m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1596\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1591\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1592\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error.response.status_code == \u001b[32m401\u001b[39m\n\u001b[32m   1593\u001b[39m ):\n\u001b[32m   1594\u001b[39m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[32m   1595\u001b[39m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1596\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[32m   1597\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1598\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1484\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1485\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\n\u001b[32m   1486\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1487\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1401\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[39m\n\u001b[32m   1400\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1401\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1410\u001b[39m hf_raise_for_status(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:285\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:309\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    308\u001b[39m response = get_session().request(method=method, url=url, **params)\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:459\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    450\u001b[39m     message = (\n\u001b[32m    451\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    452\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    457\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m https://huggingface.co/docs/huggingface_hub/authentication\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    458\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m400\u001b[39m:\n",
      "\u001b[31mRepositoryNotFoundError\u001b[39m: 401 Client Error. (Request ID: Root=1-68015c4a-684912346b260dcf64a7a043;89375ad9-01e7-4c3c-8268-c7b55d2e7f5d)\n\nRepository Not Found for url: https://huggingface.co/gpt3/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m log_file = \u001b[33m\"\u001b[39m\u001b[33mstory_log.txt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# --- LOAD MODEL & TOKENIZER ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m model = AutoModelForCausalLM.from_pretrained(model_name)\n\u001b[32m     15\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:946\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    943\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m946\u001b[39m tokenizer_config = \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[32m    948\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = tokenizer_config[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:778\u001b[39m, in \u001b[36mget_tokenizer_config\u001b[39m\u001b[34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[39m\n\u001b[32m    775\u001b[39m     token = use_auth_token\n\u001b[32m    777\u001b[39m commit_hash = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    795\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:266\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    209\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    210\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    211\u001b[39m     **kwargs,\n\u001b[32m    212\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    213\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    215\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    264\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ihno\\Hoger onderwijs\\UCLL\\Jaar 2\\Advanced AI\\semester project\\avanced_ai_semester_project\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:456\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[32m    455\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    457\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a local folder and is not a valid model identifier \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    458\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlisted on \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    459\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    460\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`token=<your_token>`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    461\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    462\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RevisionNotFoundError):\n\u001b[32m    463\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    464\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    465\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfor this model name. Check the model page at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    466\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m for available revisions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    467\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: gpt3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "model_name = \"gpt2-medium\"\n",
    "save_file = \"game_state.json\"\n",
    "log_file = \"story_log.txt\"\n",
    "\n",
    "# --- LOAD MODEL & TOKENIZER ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# --- INITIALIZE GAME STATE ---\n",
    "player_name = \"Ihno\"\n",
    "context = \"\"\n",
    "game_memory = []\n",
    "player_stats = {}\n",
    "inventory = []\n",
    "difficulty = 1\n",
    "\n",
    "# --- GENERATE STORY ---\n",
    "def generate_story(context, player_input, difficulty):\n",
    "    prompt = (\n",
    "        \"This is a fantasy adventure story.\\n\"\n",
    "        f\"Difficulty: {['Easy', 'Medium', 'Hard'][difficulty]}\\n\"\n",
    "        f\"{context}\\n\"\n",
    "        f\"{player_name}: {player_input}\\nNarrator:\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=400,\n",
    "        temperature=0.9,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Strip the prompt part off to get only new generation\n",
    "    generated = output_text[len(prompt):].strip()\n",
    "\n",
    "    # Fallback if model echoes the prompt too closely\n",
    "    if not generated:\n",
    "        generated = output_text.split(player_input)[-1].strip()\n",
    "\n",
    "    return generated\n",
    "\n",
    "\n",
    "# --- DIFFICULTY LOGIC ---\n",
    "def adjust_difficulty(player_input):\n",
    "    global difficulty\n",
    "    if any(word in player_input.lower() for word in [\"attack\", \"fight\", \"battle\"]):\n",
    "        difficulty = min(difficulty + 1, 2)\n",
    "    elif any(word in player_input.lower() for word in [\"run\", \"talk\", \"hide\"]):\n",
    "        difficulty = max(difficulty - 1, 0)\n",
    "    return difficulty\n",
    "\n",
    "# --- SAVE/LOAD ---\n",
    "def save_game():\n",
    "    with open(save_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"context\": context,\n",
    "            \"stats\": player_stats,\n",
    "            \"inventory\": inventory,\n",
    "            \"difficulty\": difficulty,\n",
    "            \"memory\": game_memory\n",
    "        }, f, indent=4)\n",
    "    print(\"üíæ Game saved.\")\n",
    "\n",
    "def delete_save():\n",
    "    if os.path.exists(save_file):\n",
    "        os.remove(save_file)\n",
    "        print(\"üóëÔ∏è Previous save data deleted.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No save file found to delete.\")\n",
    "\n",
    "# --- START MENU ---\n",
    "game_running = False\n",
    "while not game_running:\n",
    "    choice = input(\"\\nüíæ Load game (l), üÜï New game (n), üóëÔ∏è Delete save (d), or ‚ùå Quit (q)? \").strip().lower()\n",
    "    if choice == 'l':\n",
    "        if os.path.exists(save_file):\n",
    "            with open(save_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                game_state = json.load(f)\n",
    "                context = game_state[\"context\"]\n",
    "                player_stats = game_state[\"stats\"]\n",
    "                inventory = game_state[\"inventory\"]\n",
    "                difficulty = game_state[\"difficulty\"]\n",
    "                game_memory = game_state[\"memory\"]\n",
    "            print(\"‚úÖ Game loaded.\")\n",
    "            game_running = True\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No saved game found.\")\n",
    "    elif choice == 'n':\n",
    "        context = f\"{player_name} awakens in a dark forest. A mysterious figure approaches.\"\n",
    "        game_memory = [context]\n",
    "        player_stats = {\"health\": 100, \"strength\": 10, \"gold\": 5}\n",
    "        inventory = [\"torch\"]\n",
    "        difficulty = 1\n",
    "        print(\"üÜï New game started.\")\n",
    "        game_running = True\n",
    "    elif choice == 'd':\n",
    "        delete_save()\n",
    "    elif choice == 'q':\n",
    "        print(\"üõë Exiting game.\")\n",
    "        exit()\n",
    "    else:\n",
    "        print(\"‚ùì Invalid choice.\")\n",
    "\n",
    "# --- GAME LOOP ---\n",
    "print(\"\\nüå≤ Welcome to the AI Dungeon! üå≤\")\n",
    "while True:\n",
    "    print(\"\\nüìñ Story so far:\")\n",
    "    print(context)\n",
    "    print(f\"\\nüßç {player_name}'s Inventory: {inventory}\")\n",
    "    print(f\"‚ù§Ô∏è Stats: {player_stats} | üéØ Difficulty: {['Easy', 'Medium', 'Hard'][difficulty]}\")\n",
    "\n",
    "    player_input = input(\"\\nüí¨ What do you do? (type 'save', 'quit' or your action): \").strip()\n",
    "\n",
    "    if player_input.lower() == 'quit':\n",
    "        print(\"üõë Game ended. Thanks for playing!\")\n",
    "        break\n",
    "    elif player_input.lower() == 'save':\n",
    "        save_game()\n",
    "        continue\n",
    "\n",
    "    game_memory.append(f\"{player_name}: {player_input}\")\n",
    "    adjust_difficulty(player_input)\n",
    "\n",
    "    recent_context = \"\\n\".join(game_memory[-6:])\n",
    "    new_story = generate_story(recent_context, player_input, difficulty)\n",
    "\n",
    "    context += f\"\\n{player_name}: {player_input}\\n{new_story}\"\n",
    "    game_memory.append(new_story)\n",
    "\n",
    "    # Log to file\n",
    "    with open(log_file, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(f\"{player_name}: {player_input}\\n{new_story}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draft openai gpt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï New game started.\n",
      "\n",
      "üå≤ Welcome to the AI Dungeon! üå≤\n",
      "\n",
      "üìñ Story so far:\n",
      "Ihno awakens in a dark forest. A mysterious figure approaches.\n",
      "\n",
      "üßç Ihno's Inventory: ['torch']\n",
      "‚ù§Ô∏è Stats: {'health': 100, 'strength': 10, 'gold': 5} | üéØ Difficulty: Easy\n",
      "‚ö†Ô∏è Rate limited. Retrying in 1 seconds... Attempt 1/5\n",
      "‚ö†Ô∏è Rate limited. Retrying in 2 seconds... Attempt 2/5\n",
      "‚ö†Ô∏è Rate limited. Retrying in 4 seconds... Attempt 3/5\n",
      "‚ö†Ô∏è Rate limited. Retrying in 8 seconds... Attempt 4/5\n",
      "‚ö†Ô∏è Rate limited. Retrying in 16 seconds... Attempt 5/5\n",
      "‚ùå Error during story generation: ‚ùå Rate limit exceeded after 5 attempts.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from openai import OpenAI, RateLimitError\n",
    "\n",
    "# --- LOAD API KEY ---\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"‚ùå OPENAI_API_KEY not found in .env file!\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "save_file = \"game_state.json\"\n",
    "log_file = \"story_log.txt\"\n",
    "\n",
    "# --- GAME STATE ---\n",
    "player_name = \"Ihno\"\n",
    "context = \"\"\n",
    "game_memory = []\n",
    "player_stats = {}\n",
    "inventory = []\n",
    "difficulty = 1\n",
    "\n",
    "# --- DIFFICULTY LOGIC ---\n",
    "def adjust_difficulty(player_input):\n",
    "    global difficulty\n",
    "    if any(word in player_input.lower() for word in [\"attack\", \"fight\", \"battle\"]):\n",
    "        difficulty = min(difficulty + 1, 5)\n",
    "    elif any(word in player_input.lower() for word in [\"run\", \"talk\", \"hide\"]):\n",
    "        difficulty = max(difficulty - 1, 1)\n",
    "    return difficulty\n",
    "\n",
    "def generate_story(context, player_input, difficulty):\n",
    "    prompt = (\n",
    "        \"You are a fantasy storytelling AI. \"\n",
    "        \"Continue the adventure in a vivid, immersive style. \"\n",
    "        f\"Difficulty level: {difficulty}.\\n\\n\"\n",
    "        f\"{context}\\n\"\n",
    "        f\"{player_name}: {player_input}\\n\"\n",
    "        \"Narrator:\"\n",
    "    )\n",
    "\n",
    "    max_retries = 5  # Increase the number of retries\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=100,\n",
    "                temperature=0.9,\n",
    "                top_p=0.95\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "\n",
    "        except RateLimitError as e:\n",
    "            wait_time = 2 ** attempt  # Exponential backoff\n",
    "            print(f\"‚ö†Ô∏è Rate limited. Retrying in {wait_time} seconds... Attempt {attempt+1}/{max_retries}\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "    raise Exception(f\"‚ùå Rate limit exceeded after {max_retries} attempts.\")\n",
    "\n",
    "# --- SAVE/LOAD ---\n",
    "def save_game():\n",
    "    with open(save_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"context\": context,\n",
    "            \"stats\": player_stats,\n",
    "            \"inventory\": inventory,\n",
    "            \"difficulty\": difficulty,\n",
    "            \"memory\": game_memory\n",
    "        }, f, indent=4)\n",
    "    print(\"üíæ Game saved.\")\n",
    "\n",
    "def delete_save():\n",
    "    if os.path.exists(save_file):\n",
    "        os.remove(save_file)\n",
    "        print(\"üóëÔ∏è Previous save data deleted.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No save file found to delete.\")\n",
    "\n",
    "# --- START MENU ---\n",
    "game_running = False\n",
    "while not game_running:\n",
    "    choice = input(\"\\nüíæ Load game (l), üÜï New game (n), üóëÔ∏è Delete save (d), or ‚ùå Quit (q)? \").strip().lower()\n",
    "    if choice == 'l':\n",
    "        if os.path.exists(save_file):\n",
    "            with open(save_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                game_state = json.load(f)\n",
    "                context = game_state[\"context\"]\n",
    "                player_stats = game_state[\"stats\"]\n",
    "                inventory = game_state[\"inventory\"] \n",
    "                difficulty = game_state[\"difficulty\"]\n",
    "                game_memory = game_state[\"memory\"]\n",
    "            print(\"‚úÖ Game loaded.\")\n",
    "            game_running = True\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No saved game found.\")\n",
    "    elif choice == 'n':\n",
    "        context = f\"{player_name} awakens in a dark forest. A mysterious figure approaches.\"\n",
    "        game_memory = [context]\n",
    "        player_stats = {\"health\": 100, \"strength\": 10, \"gold\": 5}\n",
    "        inventory = [\"torch\"]\n",
    "        difficulty = 1\n",
    "        print(\"üÜï New game started.\")\n",
    "        game_running = True\n",
    "    elif choice == 'd':\n",
    "        delete_save()\n",
    "    elif choice == 'q':\n",
    "        print(\"üõë Exiting game.\")\n",
    "        exit()\n",
    "    else:\n",
    "        print(\"‚ùì Invalid choice.\")\n",
    "\n",
    "# --- GAME LOOP ---\n",
    "print(\"\\nüå≤ Welcome to the AI Dungeon! üå≤\")\n",
    "while True:\n",
    "    print(\"\\nüìñ Story so far:\")\n",
    "    print(context)\n",
    "    print(f\"\\nüßç {player_name}'s Inventory: {inventory}\")\n",
    "    print(f\"‚ù§Ô∏è Stats: {player_stats} | üéØ Difficulty: {['Easy', 'Medium', 'Hard', 'Very Hard', 'Nightmare'][difficulty - 1]}\")\n",
    "\n",
    "    player_input = input(\"\\nüí¨ What do you do? (type 'save', 'quit' or your action): \").strip()\n",
    "\n",
    "    if player_input.lower() == 'quit':\n",
    "        print(\"üõë Game ended. Thanks for playing!\")\n",
    "        break\n",
    "    elif player_input.lower() == 'save':\n",
    "        save_game()\n",
    "        continue\n",
    "\n",
    "    game_memory.append(f\"{player_name}: {player_input}\")\n",
    "    adjust_difficulty(player_input)\n",
    "\n",
    "    recent_context = \"\\n\".join(game_memory[-6:])\n",
    "    try:\n",
    "        new_story = generate_story(recent_context, player_input, difficulty)\n",
    "        context += f\"\\n{player_name}: {player_input}\\n{new_story}\"\n",
    "        game_memory.append(new_story)\n",
    "\n",
    "        # Log to file\n",
    "        with open(log_file, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"{player_name}: {player_input}\\n{new_story}\\n\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during story generation: {e}\")\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
